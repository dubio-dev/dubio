project_root: "."

wandb:
  project: "JAM"
  name: "pretrain"
  mode: online
  
# Training-related parameters
training:
  resume_from_safetensors: ""
  grad_accumulation_steps: 1
  max_steps: 750000
  learning_rate: 7.5e-5
  num_warmup_updates: 10000
  save_per_updates: 10000
  last_per_steps: 10000
  log_every: 50
  checkpoint_path: "outputs/sft"
  max_grad_norm: 10

# Model and data parameters
model:
  num_channels: 64
  cfm:
    max_frames: ${max_frames}
    num_channels: ${model.num_channels}
    dual_drop_prob: [0.1, 0.5]
    no_edit: true
  dit:
    max_frames: ${max_frames}
    mel_dim: ${model.num_channels}
    dim: 1408
    depth: 16
    heads: 32
    ff_mult: 4
    text_dim: 512
    conv_layers: 4
    use_implicit_duration: true
    grad_ckpt: false

data:
  train_dataset:
    pattern: "your_jam_dataset-{000000..000485}.tar"
    id_list_jsonl: "your_jam_dataset.jsonl"
    max_frames: ${max_frames}
    multiple_styles: true
    sampling_rate: 44100
    resample_by_duration_threshold: 150
    shuffle: true
    silence_latent_path: "public/silence_latent.pt"
    tokenizer_path: "public/en_us_cmudict_ipa_forward.pt"
    lrc_upsample_factor: ${lrc_upsample_factor}
    filler: average_sparse

  train_dataloader:
    batch_size: 4
    num_workers: 4
    persistent_workers: true
    pin_memory: true
    prefetch_factor: 2

# General settings
max_frames: 2000
lrc_upsample_factor: 4
seed: 42